{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Αντίγραφο ex_2b.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBiPH8i1z15Z"
      },
      "source": [
        "##Exercise 2c\r\n",
        "###Multilayer Perceptron \r\n",
        "####Two hidden layers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2l7veWniVNS"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "import random"
      ],
      "execution_count": 394,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_P5o5zSifen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb7807e-8f8f-4a5b-834c-a5184eb3cd85"
      },
      "source": [
        "iris = pd.read_csv(\"iris.csv\")\r\n",
        "random.seed(6)\r\n",
        "iris = iris.sample(frac=1,random_state=5).reset_index(drop=True) \r\n",
        "\r\n",
        "print (\"Random number with seed 6\")\r\n",
        "random.seed(6)"
      ],
      "execution_count": 395,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random number with seed 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mon7vvjDiqRj"
      },
      "source": [
        "X = iris[['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']]\r\n",
        "X = np.array(X)"
      ],
      "execution_count": 396,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22BiELzdi_id"
      },
      "source": [
        "\r\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\r\n",
        "Y = iris.Species\r\n",
        "Y = one_hot_encoder.fit_transform(np.array(Y).reshape(-1, 1))\r\n"
      ],
      "execution_count": 397,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0Fq1bS_jZI_"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15)\r\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1)"
      ],
      "execution_count": 398,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUvtS9s9ke6N"
      },
      "source": [
        "def InitializeWeights(nodes):\r\n",
        "    \"\"\"Initialize weights with random values in [-1, 1] (including bias)\"\"\"\r\n",
        "    np.random.seed(6)\r\n",
        "    layers, weights = len(nodes), []\r\n",
        "    \r\n",
        "    for i in range(1, layers):\r\n",
        "        np.random.seed(6)\r\n",
        "        w = [[np.random.uniform(-1, 1) for k in range(nodes[i-1] + 1)]\r\n",
        "              for j in range(nodes[i])]\r\n",
        "        weights.append(np.matrix(w))\r\n",
        "    print(weights)\r\n",
        "    return weights"
      ],
      "execution_count": 399,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTB80UTnlgHa"
      },
      "source": [
        "def Sigmoid(x):\r\n",
        "    return 1 / (1 + np.exp(-x))\r\n",
        "\r\n",
        "def SigmoidDerivative(x):\r\n",
        "    return np.multiply(x, 1-x)"
      ],
      "execution_count": 400,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De-8--Nnlk-d"
      },
      "source": [
        "def ForwardPropagation(x, weights, layers):\r\n",
        "    activations, layer_input = [x], x #initializing with the input\r\n",
        "    #in every iteration of j layer_input is the sigmoid of the previous layer\r\n",
        "    for j in range(layers):\r\n",
        "        activation = Sigmoid(np.dot(layer_input, weights[j].T))\r\n",
        "        activations.append(activation)\r\n",
        "        layer_input = np.append(1, activation) # Augment with bias\r\n",
        "    \r\n",
        "    return activations"
      ],
      "execution_count": 401,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr2wl0VFmmOz"
      },
      "source": [
        "def BackPropagation(y, activations, weights, layers):\r\n",
        "    outputFinal = activations[-1]\r\n",
        "    error = np.matrix(y - outputFinal) # Error at output\r\n",
        "    #backprop layers from last to first\r\n",
        "    for j in range(layers, 0, -1):\r\n",
        "        currActivation = activations[j]\r\n",
        "        \r\n",
        "        if(j > 1):\r\n",
        "            # Augment previous activation\r\n",
        "            prevActivation = np.append(1, activations[j-1])\r\n",
        "        else:\r\n",
        "            # First hidden layer, prevActivation is input (without bias)\r\n",
        "            prevActivation = activations[0]\r\n",
        "        \r\n",
        "        delta = np.multiply(error, SigmoidDerivative(currActivation))\r\n",
        "        weights[j-1] += lr * np.multiply(delta.T, prevActivation)\r\n",
        "\r\n",
        "        w = np.delete(weights[j-1], [0], axis=1) # Remove bias from weights\r\n",
        "        error = np.dot(delta, w) # Calculate error for current layer\r\n",
        "    \r\n",
        "    return weights"
      ],
      "execution_count": 402,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwHcznHimo3n"
      },
      "source": [
        "def Train(X, Y, lr, weights):\r\n",
        "    layers = len(weights)\r\n",
        "    for i in range(len(X)):\r\n",
        "        x, y = X[i], Y[i]\r\n",
        "        x = np.matrix(np.append(1, x)) # Augment feature vector\r\n",
        "        \r\n",
        "        activations = ForwardPropagation(x, weights, layers)\r\n",
        "        weights = BackPropagation(y, activations, weights, layers)\r\n",
        "\r\n",
        "    return weights"
      ],
      "execution_count": 403,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQrsNZ_NmuO5"
      },
      "source": [
        "def Predict(item, weights):\r\n",
        "    layers = len(weights)\r\n",
        "    item = np.append(1, item) # Augment feature vector\r\n",
        "    \r\n",
        "    ##_Forward Propagation_##\r\n",
        "    activations = ForwardPropagation(item, weights, layers)\r\n",
        "    \r\n",
        "    outputFinal = activations[-1].A1\r\n",
        "    index = FindMaxActivation(outputFinal)\r\n",
        "\r\n",
        "    # Initialize prediction vector to zeros\r\n",
        "    y = [0 for i in range(len(outputFinal))]\r\n",
        "    y[index] = 1  # Set guessed class to 1\r\n",
        "\r\n",
        "    return y # Return prediction vector\r\n",
        "\r\n",
        "\r\n",
        "def FindMaxActivation(output):\r\n",
        "    \"\"\"Find max activation in output\"\"\"\r\n",
        "    m, index = output[0], 0\r\n",
        "    for i in range(1, len(output)):\r\n",
        "        if(output[i] > m):\r\n",
        "            m, index = output[i], i\r\n",
        "    \r\n",
        "    return index"
      ],
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t35SAhVmwZq"
      },
      "source": [
        "def Accuracy(X, Y, weights):\r\n",
        "    \"\"\"Run set through network, find overall accuracy\"\"\"\r\n",
        "    correct = 0\r\n",
        "\r\n",
        "    for i in range(len(X)):\r\n",
        "        x, y = X[i], list(Y[i])\r\n",
        "        guess = Predict(x, weights)\r\n",
        "\r\n",
        "        if(y == guess):\r\n",
        "            # Guessed correctly\r\n",
        "            correct += 1\r\n",
        "\r\n",
        "    return correct / len(X)"
      ],
      "execution_count": 405,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0uOtOjHjfBt"
      },
      "source": [
        "def NeuralNetwork(X_train, Y_train, X_val=None, Y_val=None, epochs=10, nodes=[], lr=0.15):\r\n",
        "    hidden_layers = len(nodes) - 1\r\n",
        "    weights = InitializeWeights(nodes)\r\n",
        "\r\n",
        "    for epoch in range(1, epochs+1):\r\n",
        "        weights = Train(X_train, Y_train, lr, weights)\r\n",
        "\r\n",
        "        if(epoch % 5 == 0):\r\n",
        "            print(\"Epoch {}\".format(epoch))\r\n",
        "            print(\"Training Accuracy:{}\".format(Accuracy(X_train, Y_train, weights)))\r\n",
        "            if X_val.any():\r\n",
        "                print(\"Validation Accuracy:{}\".format(Accuracy(X_val, Y_val, weights)))\r\n",
        "            if Accuracy(X_val, Y_val, weights)>0.93:\r\n",
        "              break\r\n",
        "    return weights"
      ],
      "execution_count": 406,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVGkgicjmzed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aeb5303-514d-4f37-9f65-0a40d0459328"
      },
      "source": [
        "f = len(X[0]) # Number of features\r\n",
        "o = len(Y[0]) # Number of outputs / classes\r\n",
        "\r\n",
        "layers = [f, 8,5, o] # Number of nodes in layers\r\n",
        "lr, epochs = 0.1, 100\r\n",
        "\r\n",
        "weights = NeuralNetwork(X_train, Y_train, X_val, Y_val, epochs=epochs, nodes=layers, lr=lr);"
      ],
      "execution_count": 407,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[matrix([[ 0.7857203 , -0.33604039,  0.64245825, -0.91660675, -0.78468664],\n",
            "        [ 0.19010413,  0.05963472, -0.16238514, -0.3291843 ,  0.24503886],\n",
            "        [-0.12371715,  0.47176421,  0.03607282,  0.1577172 ,  0.29071019],\n",
            "        [ 0.98044854,  0.63971639, -0.17359813,  0.75253531,  0.64751887],\n",
            "        [-0.89105098,  0.43727447,  0.60434112,  0.47281329,  0.4182635 ],\n",
            "        [ 0.08187371, -0.75035165,  0.91529459, -0.1934874 , -0.56609768],\n",
            "        [ 0.43455169,  0.98841488, -0.48877189,  0.34261886,  0.19801183],\n",
            "        [ 0.43466429,  0.87469907, -0.29638046, -0.49273181, -0.19505498]]), matrix([[ 0.7857203 , -0.33604039,  0.64245825, -0.91660675, -0.78468664,\n",
            "          0.19010413,  0.05963472, -0.16238514, -0.3291843 ],\n",
            "        [ 0.24503886, -0.12371715,  0.47176421,  0.03607282,  0.1577172 ,\n",
            "          0.29071019,  0.98044854,  0.63971639, -0.17359813],\n",
            "        [ 0.75253531,  0.64751887, -0.89105098,  0.43727447,  0.60434112,\n",
            "          0.47281329,  0.4182635 ,  0.08187371, -0.75035165],\n",
            "        [ 0.91529459, -0.1934874 , -0.56609768,  0.43455169,  0.98841488,\n",
            "         -0.48877189,  0.34261886,  0.19801183,  0.43466429],\n",
            "        [ 0.87469907, -0.29638046, -0.49273181, -0.19505498,  0.49302143,\n",
            "          0.44814113, -0.18778441,  0.9787597 , -0.09900144]]), matrix([[ 0.7857203 , -0.33604039,  0.64245825, -0.91660675, -0.78468664,\n",
            "          0.19010413],\n",
            "        [ 0.05963472, -0.16238514, -0.3291843 ,  0.24503886, -0.12371715,\n",
            "          0.47176421],\n",
            "        [ 0.03607282,  0.1577172 ,  0.29071019,  0.98044854,  0.63971639,\n",
            "         -0.17359813]])]\n",
            "Epoch 5\n",
            "Training Accuracy:0.37719298245614036\n",
            "Validation Accuracy:0.23076923076923078\n",
            "Epoch 10\n",
            "Training Accuracy:0.6578947368421053\n",
            "Validation Accuracy:0.8461538461538461\n",
            "Epoch 15\n",
            "Training Accuracy:0.8596491228070176\n",
            "Validation Accuracy:0.9230769230769231\n",
            "Epoch 20\n",
            "Training Accuracy:0.7017543859649122\n",
            "Validation Accuracy:0.38461538461538464\n",
            "Epoch 25\n",
            "Training Accuracy:0.7017543859649122\n",
            "Validation Accuracy:0.38461538461538464\n",
            "Epoch 30\n",
            "Training Accuracy:0.7017543859649122\n",
            "Validation Accuracy:0.38461538461538464\n",
            "Epoch 35\n",
            "Training Accuracy:0.7105263157894737\n",
            "Validation Accuracy:0.38461538461538464\n",
            "Epoch 40\n",
            "Training Accuracy:0.9824561403508771\n",
            "Validation Accuracy:0.9230769230769231\n",
            "Epoch 45\n",
            "Training Accuracy:0.9736842105263158\n",
            "Validation Accuracy:0.9230769230769231\n",
            "Epoch 50\n",
            "Training Accuracy:0.9649122807017544\n",
            "Validation Accuracy:0.9230769230769231\n",
            "Epoch 55\n",
            "Training Accuracy:0.9649122807017544\n",
            "Validation Accuracy:0.8461538461538461\n",
            "Epoch 60\n",
            "Training Accuracy:0.9298245614035088\n",
            "Validation Accuracy:0.7692307692307693\n",
            "Epoch 65\n",
            "Training Accuracy:0.9298245614035088\n",
            "Validation Accuracy:0.7692307692307693\n",
            "Epoch 70\n",
            "Training Accuracy:0.9298245614035088\n",
            "Validation Accuracy:0.7692307692307693\n",
            "Epoch 75\n",
            "Training Accuracy:0.9298245614035088\n",
            "Validation Accuracy:0.7692307692307693\n",
            "Epoch 80\n",
            "Training Accuracy:0.9298245614035088\n",
            "Validation Accuracy:0.7692307692307693\n",
            "Epoch 85\n",
            "Training Accuracy:0.9385964912280702\n",
            "Validation Accuracy:0.7692307692307693\n",
            "Epoch 90\n",
            "Training Accuracy:0.9385964912280702\n",
            "Validation Accuracy:0.7692307692307693\n",
            "Epoch 95\n",
            "Training Accuracy:0.9385964912280702\n",
            "Validation Accuracy:0.7692307692307693\n",
            "Epoch 100\n",
            "Training Accuracy:0.9385964912280702\n",
            "Validation Accuracy:0.7692307692307693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLFNC5ZGm3EG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd9220d-bca2-434e-af7b-a987c2b069e6"
      },
      "source": [
        "print(\"Testing Accuracy: {}\".format(Accuracy(X_test, Y_test, weights)))"
      ],
      "execution_count": 408,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy: 0.9565217391304348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSSV-S0lJYoX"
      },
      "source": [
        "#Results\r\n",
        "##20 epochs\r\n",
        "###For 4 nodes in 1rst hidden layer and 8 in the 2nd we have :\r\n",
        "Training Accuracy:0.6578<BR>\r\n",
        "Validation Accuracy:0.7692<BR>\r\n",
        "Testing Accuracy: 0.6521<br>\r\n",
        "###For 4 nodes in 1rst hidden layer and 4 in the 2nd we have :\r\n",
        "Training Accuracy:0.7017<br>\r\n",
        "Validation Accuracy:0.5384<br>\r\n",
        "Testing Accuracy: 0.5658<br>\r\n",
        "###For 8 nodes in 1rst hidden layer and 3 in the 2nd we have :\r\n",
        "Training Accuracy:0.6754<br>\r\n",
        "Validation Accuracy:0.6923<br>\r\n",
        "Testing Accuracy: 0.6086<br>\r\n",
        "###For 8 nodes in 1rst hidden layer and 5 in the 2nd we have :\r\n",
        "Training Accuracy:0.8771<br>\r\n",
        "Validation Accuracy:0.9230<br>\r\n",
        "Testing Accuracy: 1<br>\r\n",
        "###For 8 nodes in 1rst hidden layer and 6 in the 2nd we have :\r\n",
        "Training Accuracy:0.8642<br>\r\n",
        "Validation Accuracy:0.8461<br>\r\n",
        "Testing Accuracy: 0.9561<br>\r\n",
        "#Conclusion\r\n",
        "####As we expected the model performs better than the linear-perceptron(ex_2a) since it combines the hidden layer's outputs to produce the final result.<br>The model with the 8 nodes in the 1rst hidden layer and 6 in the 2nd performs better than any other model(for the same number of epochs) which tells us that the model is not overfiting.If we increase the nodes more the accuracy decreases and the model overfits.<br><br>Each neuron in the first hidden layer of the MLP works as an independent perceptron, i.e. each hidden neuron can make linear separations. The second hidden layer can aggregate these independent linear separations into nonlinear separations.Additional hidden layers can aggregate their predecessor hidden layer's separations to complex regions in space.A further property of a hidden layer, is a kind of dimensionality reduction. The high-dimensional input data is reduced into a much lower dimensional data, by letting the neural net finding linear dependencies within the daty automatically.<br><br>However, if we create an MLP with more neurons than needed then the model can easily overfit.In general most classification problems can be solved with 1 hidden layer.\r\n",
        "\r\n",
        "\r\n"
      ]
    }
  ]
}